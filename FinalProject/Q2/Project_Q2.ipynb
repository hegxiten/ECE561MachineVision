{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "u2P8rerJEvB-",
    "outputId": "5a25afd3-9c44-4d0e-f860-b5b3e7cb5a47"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\wangz/.cache\\torch\\hub\\pytorch_vision_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size: 2413\n",
      "Testing Data Size: 805\n"
     ]
    }
   ],
   "source": [
    "# Use the testing mode for evaluation (the whole ready-to-go model), since we \n",
    "# are not contributing to the model itself\n",
    "# If user would like to use this acceleration, select the menu option \n",
    "# \"Runtime\" -> \"Change runtime type\", select \"Hardware Accelerator\" -> \"GPU\" and\n",
    "# click \"SAVE\"\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from torchvision.models.resnet import ResNet, BasicBlock\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "import inspect\n",
    "import time\n",
    "from torch import nn, optim\n",
    "import torch, os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Resize, CenterCrop, Grayscale\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "from random import sample\n",
    "\n",
    "# selected ResNet34 as the model\n",
    "model = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)\n",
    "classes = [\"Apple Golden 1\", \"Apple Golden 2\", \"Apple Golden 3\", \"Apple Granny Smith\", \"Apple Pink Lady\"]\n",
    "\n",
    "def gettrainfiles(): \n",
    "    _files, classidx, _filefullpath = [], [], []\n",
    "    for cls in range(len(classes)):\n",
    "        classidx += [cls] * len(os.listdir(traindir + '\\\\' + classes[cls]))\n",
    "        _files += os.listdir(traindir + '\\\\' + classes[cls])\n",
    "        for f in os.listdir(traindir + '\\\\' + classes[cls]):\n",
    "            _filefullpath.append(traindir + '\\\\' + classes[cls] + '\\\\' + f)\n",
    "    return [_files, classidx, _filefullpath]\n",
    "def gettestfiles(): \n",
    "    _files, classidx, _filefullpath = [], [], []\n",
    "    for cls in range(len(classes)):\n",
    "        classidx += [cls] * len(os.listdir(testdir + '\\\\' + classes[cls]))\n",
    "        _files += os.listdir(testdir + '\\\\' + classes[cls])\n",
    "        for f in os.listdir(testdir + '\\\\' + classes[cls]):\n",
    "            _filefullpath.append(testdir + '\\\\' + classes[cls] + '\\\\' + f)\n",
    "    return [_files, classidx, _filefullpath]\n",
    "\n",
    "root_path = 'C:\\\\Users\\\\wangz\\\\Documents\\\\GitHub\\\\ECE561MachineVision\\\\FinalProject\\\\Q2\\\\fruits\\\\fruits-360_dataset\\\\fruits-360'\n",
    "traindir = root_path + '\\\\Training'\n",
    "testdir = root_path + '\\\\Test'\n",
    "\n",
    "trainfiles = gettrainfiles()[0]\n",
    "trainclasses = gettrainfiles()[1]\n",
    "trainfilefullpaths = gettrainfiles()[2]\n",
    "testfiles = gettestfiles()[0]\n",
    "testclasses = gettestfiles()[1]\n",
    "testfilefullpaths = gettestfiles()[2]\n",
    "\n",
    "print(\"Training Data Size: \" + str(len(trainfiles)))\n",
    "print(\"Testing Data Size: \" + str(len(testfiles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognition Experiment Using ImageNet pre-trained model on the Fruit360 Data:\n",
      "\t True Fruit Class \t Predicted ImageNet Class\n",
      "\t ################ \t ########################\n",
      "\n",
      "\t Apple Golden 3 \t\t Granny Smith\n",
      "\t Apple Golden 2 \t\t Granny Smith\n",
      "\t Apple Pink Lady \t\t Granny Smith\n",
      "\t Apple Pink Lady \t\t Granny Smith\n",
      "\t Apple Golden 2 \t\t Granny Smith\n",
      "\t Apple Golden 3 \t\t Granny Smith\n",
      "\t Apple Granny Smith \t\t Granny Smith\n",
      "\t Apple Pink Lady \t\t orange\n",
      "\t Apple Granny Smith \t\t Granny Smith\n",
      "\t Apple Pink Lady \t\t Granny Smith\n",
      "\t Apple Golden 2 \t\t Granny Smith\n",
      "\t Apple Golden 2 \t\t Granny Smith\n",
      "\t Apple Golden 2 \t\t Granny Smith\n",
      "\t Apple Golden 1 \t\t lemon\n",
      "\t Apple Granny Smith \t\t Granny Smith\n",
      "\t Apple Golden 2 \t\t Granny Smith\n",
      "\t Apple Golden 2 \t\t Granny Smith\n",
      "\t Apple Pink Lady \t\t pomegranate\n",
      "\t Apple Golden 3 \t\t Granny Smith\n",
      "\t Apple Golden 1 \t\t lemon\n",
      "\t Apple Pink Lady \t\t orange\n",
      "\t Apple Golden 1 \t\t custard apple\n",
      "\t Apple Granny Smith \t\t Granny Smith\n",
      "\t Apple Pink Lady \t\t Granny Smith\n",
      "\t Apple Golden 2 \t\t Granny Smith\n",
      "\t Apple Golden 3 \t\t Granny Smith\n",
      "\t Apple Granny Smith \t\t Granny Smith\n",
      "\t Apple Golden 2 \t\t Granny Smith\n",
      "\t Apple Golden 3 \t\t Granny Smith\n",
      "\t Apple Granny Smith \t\t Granny Smith\n",
      "\t Apple Golden 2 \t\t Granny Smith\n",
      "\t Apple Golden 2 \t\t Granny Smith\n",
      "\t Apple Pink Lady \t\t fig\n",
      "\t Apple Golden 3 \t\t Granny Smith\n",
      "\t Apple Golden 1 \t\t lemon\n",
      "\t Apple Pink Lady \t\t Granny Smith\n",
      "\t Apple Golden 3 \t\t Granny Smith\n",
      "\t Apple Golden 3 \t\t Granny Smith\n",
      "\t Apple Golden 3 \t\t Granny Smith\n",
      "\t Apple Golden 3 \t\t Granny Smith\n",
      "\t Apple Golden 1 \t\t lemon\n",
      "\t Apple Pink Lady \t\t pomegranate\n",
      "\t Apple Golden 3 \t\t Granny Smith\n",
      "\t Apple Golden 3 \t\t Granny Smith\n",
      "\t Apple Golden 2 \t\t Granny Smith\n",
      "\t Apple Pink Lady \t\t pomegranate\n",
      "\t Apple Golden 2 \t\t Granny Smith\n",
      "\t Apple Golden 3 \t\t Granny Smith\n",
      "\t Apple Golden 3 \t\t Granny Smith\n",
      "\t Apple Pink Lady \t\t pomegranate\n"
     ]
    }
   ],
   "source": [
    "# Q2 (a) recognition experiment:\n",
    "# here I have not yet started fine-tuning the network \n",
    "with open(root_path + '\\\\' + 'imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_true, test_pred = [], []\n",
    "sample_50_image_idx = [x for x in sample(range(len(gettestfiles()[0])),50)]\n",
    "input_images = sample_50_image_paths = [Image.open(gettestfiles()[2][x]) for x in sample_50_image_idx]\n",
    "test_true = [classes[testclasses[x]] for x in sample_50_image_idx]\n",
    "for i in range(len(input_images)):\n",
    "    preprocess = transforms.Compose([\n",
    "                  transforms.CenterCrop(224),\n",
    "                  transforms.ToTensor(),\n",
    "                  transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                       std=[0.229, 0.224, 0.225]),])\n",
    "    input_tensor = preprocess(input_images[i])\n",
    "    input_batch = input_tensor.unsqueeze(0)\n",
    "    # create a mini-batch as expected by the model\n",
    "\n",
    "    # move the input and model to GPU for speed if available\n",
    "    if torch.cuda.is_available():\n",
    "        input_batch = input_batch.to('cuda')\n",
    "        model.to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "    _, index = torch.max(output, 1)\n",
    "    # The output has unnormalized scores. To get probabilities, run a \n",
    "    # softmax on it (to normalize the scores to probability).\n",
    "    percentage = torch.nn.functional.softmax(output, dim=1)[0] * 100\n",
    "    pred_label_i = labels[index[0]]\n",
    "    test_pred.append(pred_label_i)\n",
    "\n",
    "print(\"Recognition Experiment Using ImageNet pre-trained model on the Fruit360 Data:\")\n",
    "print('\\t', \"True Fruit Class\", '\\t', \"Predicted ImageNet Class\")\n",
    "print('\\t', \"################\", '\\t', \"########################\\n\")\n",
    "\n",
    "for i in range(len(sample_50_image_idx)):\n",
    "    print('\\t', test_true[i], '\\t\\t', test_pred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional Utilities\n",
    "class FiveClassFruit(Dataset):\n",
    "    def __init__(self, filelist, classlist, fullfilepath, transform=None):\n",
    "        self.filelist = filelist\n",
    "        self.classlist = classlist\n",
    "        self.fullfilepath = fullfilepath\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        assert len(self.filelist) == len(self.classlist) == len(self.fullfilepath)\n",
    "        return len(self.filelist)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.filelist[idx]            \n",
    "        fullname = self.fullfilepath[idx]\n",
    "        cls = self.classlist[idx]\n",
    "        image = Image.open(fullname)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return [image, cls]\n",
    "\n",
    "def get_data_loaders(train_batch_size=8, val_batch_size=2):    \n",
    "    data_transform = Compose([Resize((224,224)),\n",
    "                              CenterCrop(224),\n",
    "                              ToTensor(), \n",
    "                              Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                        std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "    train_loader = DataLoader(FiveClassFruit(trainfiles, \n",
    "                                             trainclasses,\n",
    "                                             trainfilefullpaths, \n",
    "                                             transform=data_transform),\n",
    "                              batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "    val_loader = DataLoader(FiveClassFruit(testfiles, \n",
    "                                           testclasses,\n",
    "                                           testfilefullpaths, \n",
    "                                           transform=data_transform),\n",
    "                            batch_size=val_batch_size, shuffle=False)\n",
    "    # use the train dataset for both train and validation (no labels on testset)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def calculate_metric(metric_fn, true_y, pred_y):\n",
    "    # multi class problems need to have averaging method\n",
    "    if \"average\" in inspect.getfullargspec(metric_fn).args:\n",
    "        return metric_fn(true_y, pred_y, average=\"macro\")\n",
    "    else:\n",
    "        return metric_fn(true_y, pred_y)\n",
    "    \n",
    "def print_scores(p, r, f1, a, batch_size):\n",
    "    for name, scores in zip((\"precision\", \"recall\", \"F1\", \"accuracy\"), (p, r, f1, a)):\n",
    "        print(f\"\\t{name.rjust(14, ' ')}: {sum(scores)/batch_size:.4f}\")\n",
    "\n",
    "def train_and_val(model):\n",
    "    batches = len(train_loader)\n",
    "    val_batches = len(val_loader)\n",
    "\n",
    "    # loop for every epoch (training + evaluation)\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        # progress bar\n",
    "        progress = tqdm(enumerate(train_loader), desc=\"Loss: \", total=batches)\n",
    "\n",
    "        # ----------------- TRAINING  -------------------- \n",
    "        # set model to training\n",
    "        model.train()\n",
    "        for i, data in progress:\n",
    "            X, y = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # training step for single batch\n",
    "            model.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = loss_function(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # getting training quality data\n",
    "            current_loss = loss.item()\n",
    "            total_loss += current_loss\n",
    "\n",
    "            # updating progress bar\n",
    "            progress.set_description(\"Loss: {:.4f}\".format(total_loss/(i+1)))\n",
    "\n",
    "        # releasing unceseccary memory in GPU\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # ----------------- VALIDATION  ----------------- \n",
    "        val_losses = 0\n",
    "        precision, recall, f1, accuracy = [], [], [], []\n",
    "        true_cls, pred_cls = [], []\n",
    "        # set model to evaluating (testing)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(val_loader):\n",
    "                X, y = data[0].to(device), data[1].to(device)\n",
    "                # Get's the prediction (outputs) from the network\n",
    "                outputs = model(X)\n",
    "                val_losses += loss_function(outputs, y)\n",
    "\n",
    "                predicted_classes = torch.max(outputs, 1)[1] # get class from network's prediction\n",
    "                true_cls.extend(y.cpu())\n",
    "                pred_cls.extend(predicted_classes.cpu())\n",
    "\n",
    "                # calculate P/R/F1/A metrics for batch\n",
    "                for acc, metric in zip((precision, recall, f1, accuracy), \n",
    "                                       (precision_score, recall_score, f1_score, accuracy_score)):\n",
    "                    acc.append(calculate_metric(metric, \n",
    "                                                y.cpu(), predicted_classes.cpu()))\n",
    "            print(confusion_matrix(true_cls, pred_cls, labels=[0,1,2,3,4]))\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, training loss: {total_loss/batches}, validation loss: {val_losses/val_batches}\")\n",
    "            print_scores(precision, recall, f1, accuracy, val_batches)\n",
    "            losses.append(total_loss/batches) # for plotting learning curve\n",
    "    print(f\"Training time: {time.time()-start_ts}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\wangz/.cache\\torch\\hub\\pytorch_vision_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained ResNet18 with fine-tuning the fully-connected layer only:\n",
      "Batch Norms are all activated as pretraied parameters:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254841208d1946acb1fabcb3c50112dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loss: ', max=38, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[164   0   0   0   0]\n",
      " [  0 164   0   0   0]\n",
      " [  0   0 161   0   0]\n",
      " [  0   0  15 149   0]\n",
      " [  0   0   0   0 152]]\n",
      "Epoch 1/3, training loss: 0.7603741319555986, validation loss: 0.2977934181690216\n",
      "\t     precision: 0.9608\n",
      "\t        recall: 0.9516\n",
      "\t            F1: 0.9553\n",
      "\t      accuracy: 0.9816\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eff8e09e1444dab95927e3420f16567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loss: ', max=38, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[164   0   0   0   0]\n",
      " [  0 164   0   0   0]\n",
      " [  0   0 161   0   0]\n",
      " [  0   0  17 147   0]\n",
      " [  0   0   0   0 152]]\n",
      "Epoch 2/3, training loss: 0.19729715546495036, validation loss: 0.15635670721530914\n",
      "\t     precision: 0.9542\n",
      "\t        recall: 0.9439\n",
      "\t            F1: 0.9480\n",
      "\t      accuracy: 0.9792\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9818a6db5d734a7baaa4b511bbda77f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loss: ', max=38, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[164   0   0   0   0]\n",
      " [  0 164   0   0   0]\n",
      " [  0   0 161   0   0]\n",
      " [  0   0   8 156   0]\n",
      " [  0   0   0   0 152]]\n",
      "Epoch 3/3, training loss: 0.1111740906183657, validation loss: 0.10285050421953201\n",
      "\t     precision: 0.9706\n",
      "\t        recall: 0.9657\n",
      "\t            F1: 0.9678\n",
      "\t      accuracy: 0.9902\n",
      "Training time: 48.58374357223511s\n"
     ]
    }
   ],
   "source": [
    "# Use ResNet18 for fine-tuning fully connected layer to achieve the best results\n",
    "model = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)\n",
    "# Fine-tuning: First modify the model configurations\n",
    "# set all weights to be fixed: not back-propogating gradients\n",
    "# set the number of classes as 5 (required by the project)\n",
    "# replace the last layer with our custmozied linear layer with 5 out features (5 classes)\n",
    "class_num = len(classes)\n",
    "channel_in = model.fc.in_features\n",
    "\n",
    "model.fc = nn.Linear(in_features=channel_in, out_features=class_num)\n",
    "for params in model.parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "# set the fully connected layer to be trainable: require back-propogation of gradients\n",
    "for params in model.fc.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "epochs = 3\n",
    "train_loader, val_loader = get_data_loaders(64,16)# put your data loader here, play with batch size to satisfy cuda\n",
    "loss_function = nn.CrossEntropyLoss() # your loss function, cross entropy works well for multi-class problems\n",
    "\n",
    "# optimizer, try Adam this time to have a taste\n",
    "# optimize the full connection layer only, magic numbers from online resource\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "start_ts = time.time()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_cuda = True\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model.cuda()\n",
    "\n",
    "losses = []\n",
    "\n",
    "print(\"Pretrained ResNet18 with fine-tuning the fully-connected layer only:\")\n",
    "print(\"Batch Norms are all activated as pretraied parameters:\")\n",
    "\n",
    "train_and_val(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\wangz/.cache\\torch\\hub\\pytorch_vision_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained ResNet18 with fine-tuning the fully-connected layer only:\n",
      "Last layer Batch Norm Children are all deactivated (replaced by Identitiy layer):\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbbfb67298444003b9937e6c137c91d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loss: ', max=38, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[164   0   0   0   0]\n",
      " [164   0   0   0   0]\n",
      " [161   0   0   0   0]\n",
      " [150   9   0   5   0]\n",
      " [140   0   0   0  12]]\n",
      "Epoch 1/10, training loss: 1.5623458341548317, validation loss: 1.5269943475723267\n",
      "\t     precision: 0.2590\n",
      "\t        recall: 0.2167\n",
      "\t            F1: 0.2163\n",
      "\t      accuracy: 0.2218\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd3b32577524114919983d946f02c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loss: ', max=38, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[164   0   0   0   0]\n",
      " [143  21   0   0   0]\n",
      " [102  26  32   1   0]\n",
      " [  3  14   0 147   0]\n",
      " [ 65   0   0   0  87]]\n",
      "Epoch 2/10, training loss: 1.4755128527942456, validation loss: 1.447490930557251\n",
      "\t     precision: 0.6007\n",
      "\t        recall: 0.4508\n",
      "\t            F1: 0.4726\n",
      "\t      accuracy: 0.5662\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff034e612cf84487815686e3597fee7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loss: ', max=38, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[164   0   0   0   0]\n",
      " [102  62   0   0   0]\n",
      " [ 34   1 119   7   0]\n",
      " [  0   0   0 164   0]\n",
      " [ 30   0   0   0 122]]\n",
      "Epoch 3/10, training loss: 1.3949814281965558, validation loss: 1.3755168914794922\n",
      "\t     precision: 0.7824\n",
      "\t        recall: 0.6875\n",
      "\t            F1: 0.7068\n",
      "\t      accuracy: 0.7868\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1fa0be5d8884f86b430d721cc7121e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loss: ', max=38, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[164   0   0   0   0]\n",
      " [ 26 138   0   0   0]\n",
      " [ 17  16 128   0   0]\n",
      " [  0   0   0 164   0]\n",
      " [ 13   0   0   0 139]]\n",
      "Epoch 4/10, training loss: 1.3215463035985042, validation loss: 1.3053209781646729\n",
      "\t     precision: 0.8207\n",
      "\t        recall: 0.7841\n",
      "\t            F1: 0.7975\n",
      "\t      accuracy: 0.9118\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed25f90e5e964df09343e8d66f7d8730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loss: ', max=38, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[164   0   0   0   0]\n",
      " [ 17 147   0   0   0]\n",
      " [ 12   8 137   4   0]\n",
      " [  0   0   0 164   0]\n",
      " [  6   0   0   0 146]]\n",
      "Epoch 5/10, training loss: 1.2552130786996138, validation loss: 1.242188572883606\n",
      "\t     precision: 0.8395\n",
      "\t        recall: 0.8157\n",
      "\t            F1: 0.8251\n",
      "\t      accuracy: 0.9424\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786cc4ff2afe4021991203a2b0fcf63f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loss: ', max=38, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[164   0   0   0   0]\n",
      " [  2 162   0   0   0]\n",
      " [  7  11 140   3   0]\n",
      " [  0   0   0 164   0]\n",
      " [  3   0   0   0 149]]\n",
      "Epoch 6/10, training loss: 1.1926348711314954, validation loss: 1.185664176940918\n",
      "\t     precision: 0.8922\n",
      "\t        recall: 0.8795\n",
      "\t            F1: 0.8848\n",
      "\t      accuracy: 0.9681\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76eb67fb7065407fb69e7c3003183041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loss: ', max=38, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[164   0   0   0   0]\n",
      " [  2 162   0   0   0]\n",
      " [  6   2 153   0   0]\n",
      " [  0   0   0 164   0]\n",
      " [  4   0   0   0 148]]\n",
      "Epoch 7/10, training loss: 1.1342423432751705, validation loss: 1.1340574026107788\n",
      "\t     precision: 0.9346\n",
      "\t        recall: 0.9259\n",
      "\t            F1: 0.9298\n",
      "\t      accuracy: 0.9828\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788916067f91434ba787bbd9d2a025a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loss: ', max=38, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[164   0   0   0   0]\n",
      " [  1 163   0   0   0]\n",
      " [  5  10 146   0   0]\n",
      " [  0   0   0 164   0]\n",
      " [  0   0   0   0 152]]\n",
      "Epoch 8/10, training loss: 1.0810902369649786, validation loss: 1.0807461738586426\n",
      "\t     precision: 0.9412\n",
      "\t        recall: 0.9326\n",
      "\t            F1: 0.9362\n",
      "\t      accuracy: 0.9804\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c990ce1846cb48938cb1dd5aeeb6874e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loss: ', max=38, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[164   0   0   0   0]\n",
      " [  0 164   0   0   0]\n",
      " [  3   6 150   2   0]\n",
      " [  0   0   0 164   0]\n",
      " [  0   0   0   0 152]]\n",
      "Epoch 9/10, training loss: 1.0292262874151532, validation loss: 1.0355697870254517\n",
      "\t     precision: 0.9641\n",
      "\t        recall: 0.9594\n",
      "\t            F1: 0.9613\n",
      "\t      accuracy: 0.9865\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c276b164994266b432acac5af00a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loss: ', max=38, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[164   0   0   0   0]\n",
      " [  0 164   0   0   0]\n",
      " [  4   5 152   0   0]\n",
      " [  0   0   0 164   0]\n",
      " [  0   0   0   0 152]]\n",
      "Epoch 10/10, training loss: 0.9820238289080168, validation loss: 0.9894592761993408\n",
      "\t     precision: 0.9673\n",
      "\t        recall: 0.9630\n",
      "\t            F1: 0.9648\n",
      "\t      accuracy: 0.9890\n",
      "Training time: 161.34204483032227s\n"
     ]
    }
   ],
   "source": [
    "# Use ResNet18 for testing Batch Normalization\n",
    "# ResNet pretrained models already uses BatchNorm at each layer\n",
    "# At this experiment, deactivate some BatchNorm children layers to see the performance difference\n",
    "# Still need to modify the fc layer because we are dealing with a five-class classification problem\n",
    "model = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)\n",
    "# set the number of classes as 5 (required by the project)\n",
    "# replace the last layer with our custmozied linear layer with 5 out features (5 classes)\n",
    "class_num = len(classes)\n",
    "channel_in = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features=channel_in, out_features=class_num)\n",
    "\n",
    "# Deactivating the BatchNorms of the last layer\n",
    "model.layer4[0].bn1 = nn.Identity()\n",
    "model.layer4[0].bn2 = nn.Identity()\n",
    "model.layer4[1].bn1 = nn.Identity()\n",
    "model.layer4[1].bn2 = nn.Identity()\n",
    "\n",
    "\n",
    "# Fine-tuning: First modify the model configurations\n",
    "# set all weights to be fixed: not back-propogating gradients\n",
    "for params in model.parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "# set the fully connected layer to be trainable: require back-propogation of gradients\n",
    "for params in model.fc.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "train_loader, val_loader = get_data_loaders(64,16)# put your data loader here, play with batch size to satisfy cuda\n",
    "loss_function = nn.CrossEntropyLoss() # your loss function, cross entropy works well for multi-class problems\n",
    "\n",
    "# optimizer, try Adam this time to have a taste\n",
    "# optimize the full connection layer only, magic numbers from online resource\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "start_ts = time.time()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_cuda = True\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model.cuda()\n",
    "\n",
    "losses = []\n",
    "\n",
    "print(\"Pretrained ResNet18 with fine-tuning the fully-connected layer only:\")\n",
    "print(\"Last layer Batch Norm Children are all deactivated (replaced by Identitiy layer):\\n\")\n",
    "\n",
    "train_and_val(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1RBxcvnJm3OP",
    "outputId": "8ee48c98-7828-4d0e-f086-dc886cd5fd5c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\wangz/.cache\\torch\\hub\\pytorch_vision_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained Alexnet with fine-tuning the last linear layer only:\n",
      "Alexnet With Dropout Fully Activated:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1352da7a8ce4400a8fe324ab3de6cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loss: ', max=38, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[164   0   0   0   0]\n",
      " [  0 163   1   0   0]\n",
      " [  0  15 146   0   0]\n",
      " [  0   0  23 141   0]\n",
      " [  0   0   0   0 152]]\n",
      "Epoch 1/3, training loss: 53.99978729219813, validation loss: 0.4993555247783661\n",
      "\t     precision: 0.9216\n",
      "\t        recall: 0.8977\n",
      "\t            F1: 0.9055\n",
      "\t      accuracy: 0.9522\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b99db150af5499fbb49eb2cca409188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loss: ', max=38, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[164   0   0   0   0]\n",
      " [  0 160   4   0   0]\n",
      " [  0   3 158   0   0]\n",
      " [  0   0  40 124   0]\n",
      " [  0   0   0   0 152]]\n",
      "Epoch 2/3, training loss: 3.4772173696615747, validation loss: 0.7584784030914307\n",
      "\t     precision: 0.9314\n",
      "\t        recall: 0.9026\n",
      "\t            F1: 0.9104\n",
      "\t      accuracy: 0.9424\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f49b7a2e0a94e5586b328d32bf07880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loss: ', max=38, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[164   0   0   0   0]\n",
      " [  0 164   0   0   0]\n",
      " [  0  19 142   0   0]\n",
      " [  0   0   0 164   0]\n",
      " [  0   0   0   0 152]]\n",
      "Epoch 3/3, training loss: 1.35492720572572, validation loss: 0.367942214012146\n",
      "\t     precision: 0.9706\n",
      "\t        recall: 0.9589\n",
      "\t            F1: 0.9613\n",
      "\t      accuracy: 0.9767\n",
      "Training time: 26.92198085784912s\n"
     ]
    }
   ],
   "source": [
    "# Use Alexnet for testing Dropout Regularization\n",
    "# Alexnet already uses 0.5 in three dropout layers\n",
    "# Modify the classfier[6] of Alexnet to fine tune\n",
    "model = torch.hub.load('pytorch/vision', 'alexnet', pretrained=True)\n",
    "model.classifier[6] = nn.Linear(4096, len(classes))\n",
    "\n",
    "# set all weights to be fixed: not back-propogating gradients\n",
    "for params in model.parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "# set the classifier[6] layer to be trainable: require back-propogation of gradients\n",
    "for params in model.classifier[6].parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "epochs = 3\n",
    "train_loader, val_loader = get_data_loaders(64,16)# put your data loader here, play with batch size to satisfy cuda\n",
    "loss_function = nn.CrossEntropyLoss() # your loss function, cross entropy works well for multi-class problems\n",
    "\n",
    "# construct the stochastic gradient descent opimizer \n",
    "# and filter-out those parameters requiring gradient inputs (previsou layers)\n",
    "optimizer = torch.optim.SGD(params=filter(lambda p: p.requires_grad, model.parameters()), lr=0.1)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "start_ts = time.time()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_cuda = True\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model.cuda()\n",
    "\n",
    "losses = []\n",
    "\n",
    "print(\"Pretrained Alexnet with fine-tuning the last linear layer only:\")\n",
    "print(\"Alexnet With Dropout Fully Activated:\\n\")\n",
    "train_and_val(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rzaux5ZqX14N"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\wangz/.cache\\torch\\hub\\pytorch_vision_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained Alexnet with fine-tuning the last linear layer only:\n",
      "Alexnet With Dropout Fully Deactivated (replaced by Identity layers):\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e73b30a6be401490a73290b357bdc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loss: ', max=38, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[164   0   0   0   0]\n",
      " [  0 160   4   0   0]\n",
      " [  0   0 161   0   0]\n",
      " [  0   0  30 134   0]\n",
      " [  0   0   0   0 152]]\n",
      "Epoch 1/3, training loss: 44.93596063005297, validation loss: 0.4424959123134613\n",
      "\t     precision: 0.9412\n",
      "\t        recall: 0.9203\n",
      "\t            F1: 0.9278\n",
      "\t      accuracy: 0.9583\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f2a6d5f80346248da6c337a974b31b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loss: ', max=38, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[164   0   0   0   0]\n",
      " [  0 160   4   0   0]\n",
      " [  0   0 161   0   0]\n",
      " [  0   0  10 154   0]\n",
      " [  0   0   0   0 152]]\n",
      "Epoch 2/3, training loss: 0.044896207357707774, validation loss: 0.08289152383804321\n",
      "\t     precision: 0.9510\n",
      "\t        recall: 0.9424\n",
      "\t            F1: 0.9461\n",
      "\t      accuracy: 0.9828\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c9c607d09be47428e6d9829abefaccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loss: ', max=38, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[164   0   0   0   0]\n",
      " [  0 154  10   0   0]\n",
      " [  0   0 161   0   0]\n",
      " [  0   0  10 154   0]\n",
      " [  0   0   0   0 152]]\n",
      "Epoch 3/3, training loss: 0.0036559014640117405, validation loss: 0.11428865790367126\n",
      "\t     precision: 0.9412\n",
      "\t        recall: 0.9289\n",
      "\t            F1: 0.9340\n",
      "\t      accuracy: 0.9755\n",
      "Training time: 27.097511529922485s\n"
     ]
    }
   ],
   "source": [
    "# Use Alexnet for testing Dropout Regularization\n",
    "# Deactivating Dropout Layers by substituion of Identify layers\n",
    "# Classifier[6] still needs to be modified because the number of classes has changed to 5\n",
    "model = torch.hub.load('pytorch/vision', 'alexnet', pretrained=True)\n",
    "model.classifier[6] = nn.Linear(4096, len(classes))\n",
    "\n",
    "# deactivate the Dropout to see if there is difference or not\n",
    "model.classifier[0] = nn.Identity()\n",
    "model.classifier[3] = nn.Identity()\n",
    "\n",
    "# set all weights to be fixed: not back-propogating gradients\n",
    "for params in model.parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "# set the classifier[6] layer to be trainable: require back-propogation of gradients\n",
    "for params in model.classifier[6].parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "epochs = 3\n",
    "train_loader, val_loader = get_data_loaders(64,16)# put your data loader here, play with batch size to satisfy cuda\n",
    "loss_function = nn.CrossEntropyLoss() # your loss function, cross entropy works well for multi-class problems\n",
    "\n",
    "# construct the stochastic gradient descent opimizer \n",
    "# and filter-out those parameters requiring gradient inputs (previsou layers)\n",
    "optimizer = torch.optim.SGD(params=filter(lambda p: p.requires_grad, model.parameters()), lr=0.1)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "start_ts = time.time()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_cuda = True\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model.cuda()\n",
    "\n",
    "losses = []\n",
    "\n",
    "print(\"Pretrained Alexnet with fine-tuning the last linear layer only:\")\n",
    "print(\"Alexnet With Dropout Fully Deactivated (replaced by Identity layers):\\n\")\n",
    "train_and_val(model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW5_Q2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
