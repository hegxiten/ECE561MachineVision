{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5_Q3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hegxiten/ECE561MachineVision/blob/master/HW5_Q3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2P8rerJEvB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the testing mode for evaluation (the whole ready-to-go model), since we \n",
        "# are not contributing to the model itself\n",
        "# If user would like to use this acceleration, select the menu option \n",
        "# \"Runtime\" -> \"Change runtime type\", select \"Hardware Accelerator\" -> \"GPU\" and\n",
        "# click \"SAVE\"\n",
        "\n",
        "%matplotlib inline\n",
        "import torch,os,torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.models.resnet import ResNet, BasicBlock\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, Resize, CenterCrop, Grayscale\n",
        "\n",
        "from tqdm.autonotebook import tqdm\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
        "import inspect\n",
        "import time\n",
        "from PIL import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_MkfMHqPPHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Colab library to upload files to notebook\n",
        "from google.colab import files\n",
        "\n",
        "# Install Kaggle library\n",
        "!pip install -q kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgRJ-qirSbqB",
        "colab_type": "code",
        "outputId": "a7dc2246-502b-45d6-8697-435daa6eade4",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "# Upload kaggle API key file\n",
        "uploaded = files.upload()\n",
        "# moved "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8d9ae7b3-f130-40ed-bdc7-6721bab714fe\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-8d9ae7b3-f130-40ed-bdc7-6721bab714fe\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n",
            "'kaggle (1).json'   kaggle.json   \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEaGqdyAS4lP",
        "colab_type": "code",
        "outputId": "8673c25e-d74d-4d36-a0d5-a1b8436c65b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Mount Google Drive to access pictures\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "# Check if Google Drive is mounted\n",
        "%cd /content/gdrive\n",
        "%ls\n",
        "%cd /content/gdrive/My\\ Drive/Colab\\ Notebooks/HW5_files/dogs-vs-cats/"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive\n",
            "\u001b[0m\u001b[01;34m'My Drive'\u001b[0m/\n",
            "/content/gdrive/My Drive/Colab Notebooks/HW5_files/dogs-vs-cats\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGfWKxUkTMwm",
        "colab_type": "code",
        "outputId": "75cf469e-33ff-427e-d798-5a8967405700",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Download data for the nyc_taxi_trip_duration challenge\n",
        "!kaggle competitions download -c dogs-vs-cats"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading sampleSubmission.csv to /root\n",
            "  0% 0.00/86.8k [00:00<?, ?B/s]\n",
            "100% 86.8k/86.8k [00:00<00:00, 34.0MB/s]\n",
            "Downloading test1.zip to /root\n",
            " 97% 263M/271M [00:09<00:00, 48.1MB/s]\n",
            "100% 271M/271M [00:10<00:00, 27.8MB/s]\n",
            "Downloading train.zip to /root\n",
            " 96% 521M/543M [00:15<00:01, 22.3MB/s]\n",
            "100% 543M/543M [00:15<00:00, 36.1MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PFxxwPzTT-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fetch 10 different customized images from mounted Google Drive, \n",
        "import os\n",
        "root_path = '/content/gdrive/My Drive/Colab Notebooks/HW5_files/dogs-vs-cats/'\n",
        "traindir = root_path + 'train/'\n",
        "testdir = root_path + 'test1/'\n",
        "\n",
        "trainfiles = os.listdir(traindir)\n",
        "testfiles = os.listdir(testdir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEGS4iTP1U-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CatDogDataset(Dataset):\n",
        "    def __init__(self, filelist, root_dir, transform=None):\n",
        "        self.filelist = filelist\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.filelist)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.filelist[idx]\n",
        "        fullname = os.path.join(self.root_dir, img_name)\n",
        "        image = Image.open(fullname)\n",
        "        if \"cat\" in img_name:\n",
        "            cls = 0\n",
        "        elif \"dog\" in img_name:\n",
        "            cls = 1\n",
        "        else:\n",
        "            raise Exception(\"Input Image Not Labelled Correctly\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return [image, cls]\n",
        "\n",
        "def get_data_loaders(train_batch_size=256, val_batch_size=64):    \n",
        "    # Ugly method to hardcode convert 1 channel grayscale to 3 channel RGB-like\n",
        "    # but it is still gray input (due to MNIST from torchvision) \n",
        "    # See https://discuss.pytorch.org/t/how-to-get-mnist-data-from-torchvision-with-three-channels-for-some-pretrained-model-like-vgg/21872\n",
        "    data_transform = Compose([Resize((224,224)),\n",
        "                              CenterCrop(224),\n",
        "                              ToTensor(), \n",
        "                              Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                        std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "    train_loader = DataLoader(CatDogDataset(trainfiles, \n",
        "                                            traindir, \n",
        "                                            transform=data_transform),\n",
        "                              batch_size=train_batch_size, shuffle=True)\n",
        "\n",
        "    val_loader = DataLoader(CatDogDataset(trainfiles, \n",
        "                                          traindir, \n",
        "                                          transform=data_transform),\n",
        "                            batch_size=val_batch_size, shuffle=False)\n",
        "    # use the train dataset for both train and validation (no labels on testset)\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def calculate_metric(metric_fn, true_y, pred_y):\n",
        "    # multi class problems need to have averaging method\n",
        "    if \"average\" in inspect.getfullargspec(metric_fn).args:\n",
        "        return metric_fn(true_y, pred_y, average=\"macro\")\n",
        "    else:\n",
        "        return metric_fn(true_y, pred_y)\n",
        "    \n",
        "def print_scores(p, r, f1, a, batch_size):\n",
        "    for name, scores in zip((\"precision\", \"recall\", \"F1\", \"accuracy\"), (p, r, f1, a)):\n",
        "        print(f\"\\t{name.rjust(14, ' ')}: {sum(scores)/batch_size:.4f}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Sc4r1vAEw3y",
        "colab_type": "code",
        "outputId": "eb816e16-bb4a-49f7-c883-d49d40fe252c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "model_1 = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)\n",
        "\n",
        "NUM_CLASS = 2\n",
        "in_fc_nums = model_1.fc.in_features #resnet18 number of the last input neurons\n",
        "fc = nn.Linear(in_fc_nums,  NUM_CLASS)\n",
        "model_1.fc = fc\n",
        "\n",
        "\n",
        "epochs = 5\n",
        "train_loader, val_loader = get_data_loaders()# put your data loader here\n",
        "loss_function = nn.CrossEntropyLoss() # your loss function, cross entropy works well for multi-class problems\n",
        "\n",
        "# optimizer, try Adam this time to have a tast\n",
        "# optimize the full connection layer only, magic numbers from online resource\n",
        "optimizer = optim.Adam(model_1.fc.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
        "\n",
        "start_ts = time.time()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    model_1.cuda()\n",
        "\n",
        "losses = []\n",
        "\n",
        "batches = len(train_loader)\n",
        "val_batches = len(val_loader)\n",
        "\n",
        "# loop for every epoch (training + evaluation)\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    # progress bar\n",
        "    progress = tqdm(enumerate(train_loader), desc=\"Loss: \", total=batches)\n",
        "\n",
        "    # ----------------- TRAINING  -------------------- \n",
        "    # set model to training\n",
        "    model_1.train()\n",
        "    for i, data in progress:\n",
        "        X, y = data[0].to(device), data[1].to(device)\n",
        "        \n",
        "        # training step for single batch\n",
        "        model_1.zero_grad()\n",
        "        outputs = model_1(X)\n",
        "        loss = loss_function(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # getting training quality data\n",
        "        current_loss = loss.item()\n",
        "        total_loss += current_loss\n",
        "\n",
        "        # updating progress bar\n",
        "        progress.set_description(\"Loss: {:.4f}\".format(total_loss/(i+1)))\n",
        "        \n",
        "    # releasing unceseccary memory in GPU\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    # ----------------- VALIDATION  ----------------- \n",
        "    val_losses = 0\n",
        "    precision, recall, f1, accuracy = [], [], [], []\n",
        "    \n",
        "    # set model to evaluating (testing)\n",
        "    model_1.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(val_loader):\n",
        "            X, y = data[0].to(device), data[1].to(device)\n",
        "            # Get's the prediction (outputs) from the network\n",
        "            outputs = model_1(X) \n",
        "\n",
        "            val_losses += loss_function(outputs, y)\n",
        "\n",
        "            predicted_classes = torch.max(outputs, 1)[1] # get class from network's prediction\n",
        "            \n",
        "            # calculate P/R/F1/A metrics for batch\n",
        "            for acc, metric in zip((precision, recall, f1, accuracy), \n",
        "                                   (precision_score, recall_score, f1_score, accuracy_score)):\n",
        "                acc.append(calculate_metric(metric, \n",
        "                                            y.cpu(), predicted_classes.cpu()))\n",
        "    print(confusion_matrix(y.cpu(), predicted_classes.cpu()))\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, training loss: {total_loss/batches}, validation loss: {val_losses/val_batches}\")\n",
        "    print_scores(precision, recall, f1, accuracy, val_batches)\n",
        "    losses.append(total_loss/batches) # for plotting learning curve\n",
        "print(f\"Training time: {time.time()-start_ts}s\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 60.7MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b87a3f5ccc6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_loaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# put your data loader here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# your loss function, cross entropy works well for multi-class problems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-80de17000f18>\u001b[0m in \u001b[0;36mget_data_loaders\u001b[0;34m(train_batch_size, val_batch_size)\u001b[0m\n\u001b[1;32m     32\u001b[0m                                         std=[0.229, 0.224, 0.225])])\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     train_loader = DataLoader(CatDogDataset(trainfiles, \n\u001b[0m\u001b[1;32m     35\u001b[0m                                             \u001b[0mtraindir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                                             transform=data_transform),\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trainfiles' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhpcyZ8iCDJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_2 = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)\n",
        "NUM_CLASS = 2\n",
        "in_fc_nums = model_2.fc.in_features #resnet18 number of the last input neurons\n",
        "fc = nn.Linear(in_fc_nums,  NUM_CLASS)\n",
        "model_2.fc = fc\n",
        "\n",
        "epochs = 5\n",
        "train_loader, val_loader = get_data_loaders()# put your data loader here\n",
        "loss_function = nn.CrossEntropyLoss() # your loss function, cross entropy works well for multi-class problems\n",
        "\n",
        "# optimizer, try Adam this time to have a tast\n",
        "# optimize the full connection layer only, magic numbers from online resource\n",
        "optimizer = optim.Adam(model_2.fc.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
        "\n",
        "start_ts = time.time()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_2.cuda()\n",
        "\n",
        "losses = []\n",
        "\n",
        "batches = len(train_loader)\n",
        "val_batches = len(val_loader)\n",
        "\n",
        "# loop for every epoch (training + evaluation)\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    # progress bar\n",
        "    progress = tqdm(enumerate(train_loader), desc=\"Loss: \", total=batches)\n",
        "\n",
        "    # ----------------- TRAINING  -------------------- \n",
        "    # set model to training\n",
        "    model_2.train()\n",
        "    for i, data in progress:\n",
        "        X, y = data[0].to(device), data[1].to(device)\n",
        "        \n",
        "        # training step for single batch\n",
        "        model_2.zero_grad()\n",
        "        outputs = model_2(X)\n",
        "        loss = loss_function(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # getting training quality data\n",
        "        current_loss = loss.item()\n",
        "        total_loss += current_loss\n",
        "\n",
        "        # updating progress bar\n",
        "        progress.set_description(\"Loss: {:.4f}\".format(total_loss/(i+1)))\n",
        "        \n",
        "    # releasing unceseccary memory in GPU\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    # ----------------- VALIDATION  ----------------- \n",
        "    val_losses = 0\n",
        "    precision, recall, f1, accuracy = [], [], [], []\n",
        "    \n",
        "    # set model to evaluating (testing)\n",
        "    model_2.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(val_loader):\n",
        "            X, y = data[0].to(device), data[1].to(device)\n",
        "            # Get's the prediction (outputs) from the network\n",
        "            outputs = model_2(X) \n",
        "\n",
        "            val_losses += loss_function(outputs, y)\n",
        "\n",
        "            predicted_classes = torch.max(outputs, 1)[1] # get class from network's prediction\n",
        "            \n",
        "            # calculate P/R/F1/A metrics for batch\n",
        "            for acc, metric in zip((precision, recall, f1, accuracy), \n",
        "                                   (precision_score, recall_score, f1_score, accuracy_score)):\n",
        "                acc.append(calculate_metric(metric, \n",
        "                                            y.cpu(), predicted_classes.cpu()))\n",
        "    print(confusion_matrix(y.cpu(), predicted_classes.cpu()))\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, training loss: {total_loss/batches}, validation loss: {val_losses/val_batches}\")\n",
        "    print_scores(precision, recall, f1, accuracy, val_batches)\n",
        "    losses.append(total_loss/batches) # for plotting learning curve\n",
        "print(f\"Training time: {time.time()-start_ts}s\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RBxcvnJm3OP",
        "colab_type": "code",
        "outputId": "28e523be-36ea-489a-98cc-5ccacebd4883",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "mnist = MNIST(download=True, train=True, root=\".\").train_data.float()\n",
        "data_transform = Compose([Resize((224,224)),\n",
        "                          Grayscale(3),\n",
        "                          ToTensor(), \n",
        "                          Normalize((mnist.mean()/255,), \n",
        "                                    (mnist.std()/255,))])\n",
        "data_loader_test = DataLoader(dataset=MNIST(download=True, root=\".\", \n",
        "                                            transform=data_transform, train=True),\n",
        "                              batch_size = 200,\n",
        "                              shuffle = True)\n",
        "X_test, y_test = next(iter(data_loader_test))\n",
        "inputs = Variable(X_test.to(device))\n",
        "pred = model(inputs)\n",
        "_,pred = torch.max(pred, 1)\n",
        "print(\"Predict Label is:\", [ i for i in pred.data])\n",
        "print(\"Real Label is:\",[i for i in y_test])\n",
        "\n",
        "img = torchvision.utils.make_grid(X_test)\n",
        "img = img.numpy().transpose(1,2,0)\n",
        "\n",
        "std = [0.5,0.5,0.5]\n",
        "mean = [0.5,0.5,0.5]\n",
        "img = img*std+mean\n",
        "print(\"\\nConfusion Matrix for 10 digits from a random 200-sample validation set\")\n",
        "print(confusion_matrix(y_test, pred.cpu()))\n",
        "print(\"\\n The validation set images\")\n",
        "plt.imshow(img)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:53: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predict Label is: [tensor(8, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(7, device='cuda:0'), tensor(8, device='cuda:0'), tensor(0, device='cuda:0'), tensor(2, device='cuda:0'), tensor(4, device='cuda:0'), tensor(4, device='cuda:0'), tensor(3, device='cuda:0'), tensor(9, device='cuda:0'), tensor(2, device='cuda:0'), tensor(1, device='cuda:0'), tensor(5, device='cuda:0'), tensor(2, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(4, device='cuda:0'), tensor(3, device='cuda:0'), tensor(8, device='cuda:0'), tensor(9, device='cuda:0'), tensor(4, device='cuda:0'), tensor(2, device='cuda:0'), tensor(0, device='cuda:0'), tensor(4, device='cuda:0'), tensor(5, device='cuda:0'), tensor(4, device='cuda:0'), tensor(3, device='cuda:0'), tensor(4, device='cuda:0'), tensor(7, device='cuda:0'), tensor(1, device='cuda:0'), tensor(7, device='cuda:0'), tensor(2, device='cuda:0'), tensor(1, device='cuda:0'), tensor(2, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(2, device='cuda:0'), tensor(0, device='cuda:0'), tensor(2, device='cuda:0'), tensor(3, device='cuda:0'), tensor(7, device='cuda:0'), tensor(9, device='cuda:0'), tensor(3, device='cuda:0'), tensor(1, device='cuda:0'), tensor(6, device='cuda:0'), tensor(9, device='cuda:0'), tensor(1, device='cuda:0'), tensor(6, device='cuda:0'), tensor(0, device='cuda:0'), tensor(2, device='cuda:0'), tensor(7, device='cuda:0'), tensor(9, device='cuda:0'), tensor(0, device='cuda:0'), tensor(9, device='cuda:0'), tensor(0, device='cuda:0'), tensor(2, device='cuda:0'), tensor(7, device='cuda:0'), tensor(4, device='cuda:0'), tensor(3, device='cuda:0'), tensor(7, device='cuda:0'), tensor(7, device='cuda:0'), tensor(6, device='cuda:0'), tensor(3, device='cuda:0'), tensor(4, device='cuda:0'), tensor(4, device='cuda:0'), tensor(7, device='cuda:0'), tensor(1, device='cuda:0'), tensor(2, device='cuda:0'), tensor(2, device='cuda:0'), tensor(0, device='cuda:0'), tensor(7, device='cuda:0'), tensor(5, device='cuda:0'), tensor(6, device='cuda:0'), tensor(8, device='cuda:0'), tensor(3, device='cuda:0'), tensor(3, device='cuda:0'), tensor(2, device='cuda:0'), tensor(2, device='cuda:0'), tensor(6, device='cuda:0'), tensor(2, device='cuda:0'), tensor(8, device='cuda:0'), tensor(5, device='cuda:0'), tensor(4, device='cuda:0'), tensor(9, device='cuda:0'), tensor(0, device='cuda:0'), tensor(9, device='cuda:0'), tensor(3, device='cuda:0'), tensor(5, device='cuda:0'), tensor(1, device='cuda:0'), tensor(6, device='cuda:0'), tensor(7, device='cuda:0'), tensor(8, device='cuda:0'), tensor(1, device='cuda:0'), tensor(9, device='cuda:0'), tensor(0, device='cuda:0'), tensor(5, device='cuda:0'), tensor(3, device='cuda:0'), tensor(7, device='cuda:0'), tensor(5, device='cuda:0'), tensor(6, device='cuda:0'), tensor(7, device='cuda:0'), tensor(6, device='cuda:0'), tensor(8, device='cuda:0'), tensor(1, device='cuda:0'), tensor(3, device='cuda:0'), tensor(8, device='cuda:0'), tensor(8, device='cuda:0'), tensor(0, device='cuda:0'), tensor(3, device='cuda:0'), tensor(1, device='cuda:0'), tensor(8, device='cuda:0'), tensor(6, device='cuda:0'), tensor(7, device='cuda:0'), tensor(1, device='cuda:0'), tensor(4, device='cuda:0'), tensor(0, device='cuda:0'), tensor(2, device='cuda:0'), tensor(2, device='cuda:0'), tensor(7, device='cuda:0'), tensor(5, device='cuda:0'), tensor(2, device='cuda:0'), tensor(7, device='cuda:0'), tensor(3, device='cuda:0'), tensor(2, device='cuda:0'), tensor(5, device='cuda:0'), tensor(8, device='cuda:0'), tensor(9, device='cuda:0'), tensor(9, device='cuda:0'), tensor(9, device='cuda:0'), tensor(2, device='cuda:0'), tensor(0, device='cuda:0'), tensor(7, device='cuda:0'), tensor(3, device='cuda:0'), tensor(8, device='cuda:0'), tensor(8, device='cuda:0'), tensor(2, device='cuda:0'), tensor(1, device='cuda:0'), tensor(7, device='cuda:0'), tensor(2, device='cuda:0'), tensor(2, device='cuda:0'), tensor(1, device='cuda:0'), tensor(7, device='cuda:0'), tensor(2, device='cuda:0'), tensor(3, device='cuda:0'), tensor(9, device='cuda:0'), tensor(7, device='cuda:0'), tensor(4, device='cuda:0'), tensor(5, device='cuda:0'), tensor(5, device='cuda:0'), tensor(1, device='cuda:0'), tensor(8, device='cuda:0'), tensor(3, device='cuda:0'), tensor(7, device='cuda:0'), tensor(9, device='cuda:0'), tensor(8, device='cuda:0'), tensor(0, device='cuda:0'), tensor(3, device='cuda:0'), tensor(8, device='cuda:0'), tensor(5, device='cuda:0'), tensor(2, device='cuda:0'), tensor(4, device='cuda:0'), tensor(7, device='cuda:0'), tensor(7, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(7, device='cuda:0'), tensor(0, device='cuda:0'), tensor(9, device='cuda:0'), tensor(4, device='cuda:0'), tensor(0, device='cuda:0'), tensor(2, device='cuda:0'), tensor(5, device='cuda:0'), tensor(9, device='cuda:0'), tensor(2, device='cuda:0'), tensor(9, device='cuda:0'), tensor(8, device='cuda:0'), tensor(9, device='cuda:0'), tensor(2, device='cuda:0'), tensor(2, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(9, device='cuda:0'), tensor(1, device='cuda:0'), tensor(6, device='cuda:0'), tensor(2, device='cuda:0'), tensor(8, device='cuda:0'), tensor(3, device='cuda:0'), tensor(2, device='cuda:0'), tensor(2, device='cuda:0'), tensor(7, device='cuda:0'), tensor(9, device='cuda:0'), tensor(4, device='cuda:0'), tensor(3, device='cuda:0'), tensor(6, device='cuda:0'), tensor(1, device='cuda:0'), tensor(2, device='cuda:0'), tensor(2, device='cuda:0'), tensor(3, device='cuda:0'), tensor(8, device='cuda:0')]\n",
            "Real Label is: [tensor(8), tensor(0), tensor(0), tensor(7), tensor(8), tensor(0), tensor(2), tensor(4), tensor(4), tensor(3), tensor(9), tensor(2), tensor(1), tensor(5), tensor(2), tensor(1), tensor(0), tensor(4), tensor(3), tensor(8), tensor(9), tensor(4), tensor(2), tensor(0), tensor(4), tensor(5), tensor(4), tensor(3), tensor(4), tensor(7), tensor(1), tensor(7), tensor(2), tensor(1), tensor(2), tensor(1), tensor(0), tensor(2), tensor(0), tensor(2), tensor(3), tensor(7), tensor(9), tensor(3), tensor(1), tensor(6), tensor(9), tensor(1), tensor(6), tensor(0), tensor(2), tensor(7), tensor(4), tensor(0), tensor(9), tensor(0), tensor(7), tensor(7), tensor(4), tensor(3), tensor(7), tensor(7), tensor(6), tensor(3), tensor(4), tensor(4), tensor(7), tensor(1), tensor(2), tensor(2), tensor(0), tensor(7), tensor(5), tensor(6), tensor(8), tensor(3), tensor(3), tensor(2), tensor(2), tensor(6), tensor(2), tensor(8), tensor(5), tensor(4), tensor(9), tensor(0), tensor(9), tensor(3), tensor(5), tensor(1), tensor(6), tensor(7), tensor(8), tensor(1), tensor(9), tensor(0), tensor(5), tensor(3), tensor(7), tensor(5), tensor(6), tensor(7), tensor(6), tensor(8), tensor(1), tensor(3), tensor(8), tensor(8), tensor(0), tensor(3), tensor(1), tensor(8), tensor(6), tensor(7), tensor(1), tensor(4), tensor(0), tensor(2), tensor(2), tensor(7), tensor(5), tensor(2), tensor(7), tensor(3), tensor(2), tensor(5), tensor(8), tensor(9), tensor(9), tensor(9), tensor(2), tensor(0), tensor(7), tensor(3), tensor(8), tensor(8), tensor(2), tensor(1), tensor(7), tensor(2), tensor(2), tensor(1), tensor(7), tensor(2), tensor(3), tensor(9), tensor(7), tensor(4), tensor(5), tensor(5), tensor(1), tensor(8), tensor(3), tensor(7), tensor(9), tensor(8), tensor(0), tensor(3), tensor(8), tensor(5), tensor(2), tensor(4), tensor(7), tensor(7), tensor(1), tensor(1), tensor(7), tensor(0), tensor(9), tensor(4), tensor(0), tensor(2), tensor(5), tensor(9), tensor(2), tensor(9), tensor(8), tensor(9), tensor(2), tensor(2), tensor(1), tensor(0), tensor(9), tensor(1), tensor(6), tensor(2), tensor(8), tensor(3), tensor(2), tensor(2), tensor(7), tensor(9), tensor(4), tensor(3), tensor(6), tensor(1), tensor(2), tensor(2), tensor(3), tensor(8)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Confusion Matrix for 10 digits from a random 200-sample validation set\n",
            "[[20  0  0  0  0  0  0  0  0  0]\n",
            " [ 0 21  0  0  0  0  0  0  0  0]\n",
            " [ 0  0 33  0  0  0  0  0  0  0]\n",
            " [ 0  0  0 21  0  0  0  0  0  0]\n",
            " [ 0  0  0  0 16  0  0  0  0  1]\n",
            " [ 0  0  0  0  0 13  0  0  0  0]\n",
            " [ 0  0  0  0  0  0 11  0  0  0]\n",
            " [ 0  0  1  0  0  0  0 25  0  0]\n",
            " [ 0  0  0  0  0  0  0  0 19  0]\n",
            " [ 0  0  0  0  0  0  0  0  0 19]]\n",
            "\n",
            " The validation set images\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f296b9af1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHQAAAD8CAYAAACmeYCSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noLUMdQKtZagTaC3D/wfBYdm5pqlpbgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}